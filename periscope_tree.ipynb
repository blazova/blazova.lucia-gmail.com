{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Periscope Table/View Dependency Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following Periscope tutorial: https://www.sisense.com/blog/automated-identification-and-graphing-of-sql-dependencies/\n",
    "\n",
    "\n",
    "### Pseudo Code\n",
    "\n",
    "1. Clone the Periscope Repository\n",
    "2. Iterate over the Files with a .sql extension\n",
    "3. Create a dictionary/table/csv with: id, category (view, table, chart), code\n",
    "3. For each file ending with a .sql extension, write the filename, category, and first word between '' and after 'FROM' or 'JOIN'\n",
    "\n",
    "\n",
    "Packages: \n",
    "\n",
    "os—used to access  the current working directory\n",
    "\n",
    "argparse—parses command line arguments\n",
    "\n",
    "re—regex library for parsing out table names from SQL\n",
    "\n",
    "csv—a library for navigating CSV files\n",
    "\n",
    "networkx—modern Python graphviz library used for both creating and visualizing the directed acyclical graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "home = os.path.expanduser(\"~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lucia/Desktop/periscope\n"
     ]
    }
   ],
   "source": [
    "cd ~/Desktop/periscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"./home/lucia/Desktop/periscope\"\n",
    "# for root,d_names,f_names in os.walk(path):\n",
    "# \tprint(root, d_names, f_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dirpath, dirs, files in os.walk(\".\"):\t \n",
    "# \tpath = dirpath.split('/')\n",
    "# \tprint( (len(path))*'---', '[',os.path.basename(dirpath),']')\n",
    "# \tfor f in files:\n",
    "# \t\tprint(len(path)*'---', f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_map = {}\n",
    "\n",
    "for dirpath, dirs, files in os.walk(\"./views/\"):\t\n",
    "    for filename in files:\n",
    "        if filename.endswith(\".sql\"):\n",
    "            fname = os.path.join(dirpath,filename)\n",
    "            with open(fname) as myfile:\n",
    "                query_map[os.path.basename(fname).split(\".\")[0]] = myfile.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WITH temp AS(\\n  SELECT q.time, SUM(cw) OVER (ORDER BY q.time ASC rows between unbounded preceding and current row) AS no_customers\\n  FROM (\\n    SELECT [dataset_customers.user_created_at:month] AS time, COUNT(DISTINCT dataset_customers.id) AS cw\\n    FROM [dataset_customers]\\n    GROUP BY 1) q)\\n\\nSELECT t1.time, (t1.no_customers+t2.no_customers)/2 as avgc\\nFROM (SELECT [DATEADD(month,-1,time):month] AS time, no_customers FROM temp) t1\\nJOIN temp t2 ON t1.time = t2.time'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_map[\"helper_avg_customers_month\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(query_map)\n",
    "\n",
    "# for key in query_map:\n",
    "#     print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_columns = ['Name', 'SQL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     with open('tree_mapping.csv', 'w') as csvfile:\n",
    "#         writer = csv.DictWriter(csvfile, fieldnames=csv_columns)\n",
    "#         writer.writeheader()\n",
    "#         for data in query_map:\n",
    "#             writer.writerow(data)\n",
    "# except IOError:\n",
    "#     print(\"I/O error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-1.0.3-cp36-cp36m-manylinux1_x86_64.whl (10.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.0 MB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /home/lucia/environments/periscope_tree/lib/python3.6/site-packages (from pandas) (2.8.1)\n",
      "Collecting numpy>=1.13.3\n",
      "  Downloading numpy-1.18.2-cp36-cp36m-manylinux1_x86_64.whl (20.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 20.2 MB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytz>=2017.2\n",
      "  Using cached pytz-2019.3-py2.py3-none-any.whl (509 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/lucia/environments/periscope_tree/lib/python3.6/site-packages (from python-dateutil>=2.6.1->pandas) (1.14.0)\n",
      "Installing collected packages: numpy, pytz, pandas\n",
      "Successfully installed numpy-1.18.2 pandas-1.0.3 pytz-2019.3\n"
     ]
    }
   ],
   "source": [
    "! pip install pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>sql</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>view_cumulative_reserves</td>\n",
       "      <td>--yes_cahce\\nselect\\n  vc.user_id\\n  , month\\n...</td>\n",
       "      <td>view</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sessions_per_period</td>\n",
       "      <td>(SELECT \\n  [created_at:aggregation] as date,\\...</td>\n",
       "      <td>view</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sepa_debit_average_runtime</td>\n",
       "      <td>SELECT \\n--charges.captured_at, charges.succes...</td>\n",
       "      <td>view</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>view_claims_metrics</td>\n",
       "      <td>--yes_cache\\nwith reserve_data as (\\n  select\\...</td>\n",
       "      <td>view</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>view_business_commissions</td>\n",
       "      <td>--yes_cache\\nwith list_of_num as (select row_n...</td>\n",
       "      <td>view</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>view_expired_credits_global</td>\n",
       "      <td>--yes_cache\\nwith received_credits as (\\n  sel...</td>\n",
       "      <td>view</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>view_cac</td>\n",
       "      <td>with\\n  helper_date as (\\n    select\\n      [d...</td>\n",
       "      <td>view</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>helper_basic_upgrade_modules_link</td>\n",
       "      <td>select\\n  upgrades.insurance_chain_id upgrade_...</td>\n",
       "      <td>view</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>helper_user_flow_type</td>\n",
       "      <td>--yes_cache \\nselect \\n  shared_flow_session_i...</td>\n",
       "      <td>view</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>view_cac_without_agreg</td>\n",
       "      <td>--yes cache\\nwith\\n  helper_date as (\\n    sel...</td>\n",
       "      <td>view</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>141 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  name  \\\n",
       "0             view_cumulative_reserves   \n",
       "1                  sessions_per_period   \n",
       "2           sepa_debit_average_runtime   \n",
       "3                  view_claims_metrics   \n",
       "4            view_business_commissions   \n",
       "..                                 ...   \n",
       "136        view_expired_credits_global   \n",
       "137                           view_cac   \n",
       "138  helper_basic_upgrade_modules_link   \n",
       "139              helper_user_flow_type   \n",
       "140             view_cac_without_agreg   \n",
       "\n",
       "                                                   sql  type  \n",
       "0    --yes_cahce\\nselect\\n  vc.user_id\\n  , month\\n...  view  \n",
       "1    (SELECT \\n  [created_at:aggregation] as date,\\...  view  \n",
       "2    SELECT \\n--charges.captured_at, charges.succes...  view  \n",
       "3    --yes_cache\\nwith reserve_data as (\\n  select\\...  view  \n",
       "4    --yes_cache\\nwith list_of_num as (select row_n...  view  \n",
       "..                                                 ...   ...  \n",
       "136  --yes_cache\\nwith received_credits as (\\n  sel...  view  \n",
       "137  with\\n  helper_date as (\\n    select\\n      [d...  view  \n",
       "138  select\\n  upgrades.insurance_chain_id upgrade_...  view  \n",
       "139  --yes_cache \\nselect \\n  shared_flow_session_i...  view  \n",
       "140  --yes cache\\nwith\\n  helper_date as (\\n    sel...  view  \n",
       "\n",
       "[141 rows x 3 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_df = pd.DataFrame(list(query_map.items()), columns=['name', 'sql'])\n",
    "tree_df['type'] = 'view'\n",
    "tree_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in tree_df:\n",
    "    object_name = tree_df.name\n",
    "    object_type = tree_df.type\n",
    "    sql = tree_df.sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we dig into the details, let’s review what we want to accomplish:\n",
    "\n",
    "SQL cleanup\n",
    "\n",
    "Lowercase everything\n",
    "\n",
    "Remove block comments\n",
    "\n",
    "Remove line comments\n",
    "\n",
    "Replace all whitespace with single spaces\n",
    "\n",
    "Get set of parents\n",
    "\n",
    "Get set of Common Table Expressions(CTEs / with statement)\n",
    "\n",
    "Remove CTEs from set of parents\n",
    "\n",
    "Remove sub-selects\n",
    "\n",
    "Clean up brackets*\n",
    "\n",
    "*Sisense uses bracketed table names for views defined in the product; e.g., [customers_view]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To achieve this, we define a set of RegEx strings:\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "REG_BLOCK_COMMENT = re.compile(\"(/\\*)[\\w\\W]*?(\\*/)\", re.I)\n",
    "REG_LINE_COMMENT = re.compile('(--.*)', re.I)\n",
    "REG_BRACKETS = re.compile(\"\\[|\\]|\\)|\\\"\", re.I)\n",
    "REG_PARENTS = re.compile(\"(?<=join\\s)+[\\S\\.\\\"\\']+|(?<=from\\s)+[\\S\\.\\\"\\']+\", re.I)\n",
    "REG_CTES = re.compile(\"(\\S+)\\sas\\W*\\(\", re.I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL cleanup\n",
    "\n",
    "def clean_sql(sql):\n",
    "   c_sql = sql.lower()  # lowercase everything (for easier match)\n",
    "   c_sql = REG_BLOCK_COMMENT.sub('', c_sql)  # remove block comments\n",
    "   c_sql = REG_LINE_COMMENT.sub('', c_sql)  # remove line comments\n",
    "   c_sql = ' '.join(c_sql.split())  # replace \\n and multi space w space\n",
    "   return c_sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c_sql = ‘ ‘.join(c_sql.split())\n",
    "\n",
    "is a little tricky. This bit splits the SQL statement into an array based on arbitrary whitespace as a delimiter, and rejoins that array with a single space as a delimiter. This is to reduce multi-spaces, line breaks, and combinations of the two down to a single space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Parents From SQL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this returns the unique set of parents per query\n",
    "def get_parents(c_sql):\n",
    "   parents = set(REG_PARENTS.findall(c_sql))  \n",
    "   return parents\n",
    "# this returns the unique set of ctes per query, so we can exclude them from the list of parents\n",
    "def get_ctes(c_sql):\n",
    "   ctes = set(REG_CTES.findall(c_sql))  \n",
    "   return ctes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # clean the sql\n",
    "def get_node_info(tree_df):\n",
    "    \n",
    "    node_info_dict = {}\n",
    "    parent_dict = {}\n",
    "\n",
    "    for sql in tree_df:\n",
    "\n",
    "       c_sql = clean_sql(sql)\n",
    "       # get the set of parents\n",
    "       parents = get_parents(c_sql)\n",
    "       # get set of ctes to exclude from parents\n",
    "       ctes = get_ctes(c_sql)\n",
    "       # remove CTES from parent dict\n",
    "       for cte in ctes:\n",
    "           parents.discard(cte)\n",
    "       # get rid of brackets in views\n",
    "       c_parents = set()\n",
    "       for parent in parents:\n",
    "           if not parent[:1] == '(':\n",
    "               c_parents.add(REG_BRACKETS.sub('', parent))\n",
    "       # add the object name and type and direct parents to the dict\n",
    "       node_info_dict[object_name] = object_type  \n",
    "       parent_dict[object_name] = c_parents\n",
    "    return (parent_dict, node_info_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Relevant Node-set\n",
    "\n",
    "def get_node_set(parent_dict, focal_node=None, direction=None):\n",
    " descendant_dict = {}  # intended to store all descendants (any generation)\n",
    " ancestor_dict = {}  # intended to store all ancestry (any generation)\n",
    " node_set = set()  # final result is stored and then returned via this set\n",
    " node_set.add(focal_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this reverses a parent tree to a child tree\n",
    "def get_child_dict(parent_dict):\n",
    "   child_dict = {}\n",
    "   for node in parent_dict:\n",
    "       for parent in parent_dict[node]:\n",
    "           if not parent in child_dict.keys():\n",
    "               child_dict[parent] = set(node)\n",
    "           else:\n",
    "               child_dict[parent].add(node)\n",
    "   return child_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    " child_dict = get_child_dict(parent_dict)  # immediate children\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "child_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    " # build descendant dict\n",
    "   for node in child_dict:\n",
    "       descendant_dict[node] = traverse_tree(node, child_dict,\n",
    "               been_done=set())\n",
    "   # build ancestor dict\n",
    "   for node in parent_dict:\n",
    "       ancestor_dict[node] = traverse_tree(node, parent_dict,\n",
    "               been_done=set())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_d_graph(parent_dict, node_set, node_type_dict):\n",
    "   G = nx.DiGraph()  # initialize graph object\n",
    "   for node in node_set:\n",
    "       # Add color nodes\n",
    "       if node_type_dict.get(node, None) == 'view':\n",
    "           G.add_node(node, color='green')\n",
    "       elif node_type_dict.get(node, None) == 'chart':\n",
    "           G.add_node(node, color='blue')\n",
    "       elif node_type_dict.get(node, None) == 'csv':\n",
    "           G.add_node(node, color='red')\n",
    "       # add edges and non-color nodes\n",
    "       for parent in parent_dict.get(node, set()):\n",
    "           G.add_edge(parent, node)\n",
    "   return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip freeze > requirements.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
